<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[NeuronGraph_website]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib/media/favicon.png</url><title>NeuronGraph_website</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Sun, 19 Oct 2025 05:25:09 GMT</lastBuildDate><atom:link href="lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Sun, 19 Oct 2025 05:25:09 GMT</pubDate><copyright><![CDATA[Surjit Das]]></copyright><ttl>60</ttl><dc:creator>Surjit Das</dc:creator><item><title><![CDATA[About]]></title><description><![CDATA[ 
 <br>header<br><br><br>Please get in touch with me via any of my social media handles]]></description><link>blog/about.html</link><guid isPermaLink="false">Blog/About.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 20 Jul 2025 05:36:03 GMT</pubDate></item><item><title><![CDATA[2025]]></title><description><![CDATA[<a class="tag" href="?query=tag:reflection" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#reflection</a> <a class="tag" href="?query=tag:technology" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#technology</a> <a class="tag" href="?query=tag:inspiration" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#inspiration</a> <a class="tag" href="?query=tag:technology" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#technology</a> 
 <br>header<br><br><br>List of all the blog entries:<br><br><br><br>]]></description><link>blog/all-writings.html</link><guid isPermaLink="false">Blog/All Writings.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 19 Oct 2025 05:24:33 GMT</pubDate></item><item><title><![CDATA[Don't Let the AI Run Ahead of Yourself: Journey to Mindful Vibe Coding]]></title><description><![CDATA[ 
 <br><br><img alt="Don't let the AI run ahead of yourself-2.jpg" src="blog/don't-let-the-ai-run-ahead-of-yourself-2.jpg" style="width: 600px; max-width: 100%;"><br><br>Over the past nine months, I've embarked on an intensive journey through the landscape of AI-assisted development, creating nine working applications using the major vibe coding tools and frameworks available—from the free tiers of Cursor and bolt.new to the premium versions of Cursor, Windsurf and Warp. What started as curiosity about these emerging tools evolved into an exploration of how artificial intelligence is reshaping software development, revealing both extraordinary possibilities and subtle dangers that could fundamentally alter how we approach our craft.<br>This article emerges from that hands-on experience—the successes that compressed months of work into hours, the failures that taught me why human oversight remains irreplaceable, and the evolving understanding of where AI assistance enhances creativity versus where it can inadvertently stifle it. These aren't theoretical observations about AI coding tools; they're insights from someone who has used these systems across diverse project types, from enhancing complex RAG-based chatbots to building intuitive map interfaces to developing demos for customer vulnerability detection.<br>As artificial intelligence transforms software development at an unprecedented pace, we find ourselves at a critical juncture where the promise of instant gratification meets the reality of sustainable engineering practices. The emergence of "vibe coding"—a term coined by OpenAI co-founder Andrej Karpathy in early 2025—represents both the incredible potential and hidden dangers of our AI-powered development future[1][2]. While these tools can dramatically accelerate innovation cycles, the central challenge remains: how do we harness their power without losing the essential human element that ensures quality, maintainability, and true understanding of our code?<br><br>The current ecosystem of AI-assisted development tools spans a remarkable spectrum, each serving different needs and philosophies. My experience across nine different projects has revealed that the tool's effectiveness depends heavily on the nature of the problem you're trying to solve.<br>At one end, we have cloud-based builders like Bolt.new and Lovable.ai that promise instant application deployment from simple natural language prompts[3][4]. I discovered this firsthand when attempting to build a "How rich are you?” personal finance application—a project that perfectly illustrated both the strengths and limitations of these platforms. Within minutes, both tools had generated beautiful, responsive user interfaces that would have taken me days to create manually. The UI components were polished, the user experience was intuitive, and the visual design exceeded my expectations.<br>However, the magic ended when the application needed to perform actual data mining and complex backend operations. The sophisticated financial calculations, data integration from multiple sources, and the nuanced logic required for accurate wealth assessment simply couldn't be generated by these platforms[5][6]. They excel at creating that magical "wow" moment where an idea becomes a working prototype in minutes, but they fall short when it comes to the complex requirements, security considerations, and architectural decisions that production software demands[4][5].<br>Moving up the sophistication ladder, we encounter serious development environments like Cursor and Windsurf, which have emerged as the current leaders in AI-assisted coding[7][8]. My experience with these tools spans both brownfield enhancement and greenfield development, revealing different strengths in each context.<br>For brownfield development, I used Cursor to enhance an existing RAG-based chatbot application—a well-architected system I had previously designed using my own patterns and principles. Cursor proved exceptional at understanding the existing code components (when pointed to them) and organically integrating new features like GraphRAG capabilities and AdaptiveRAG concepts that the LLMs had learnt on their pre-training. The AI could maintain consistency with my established patterns while introducing sophisticated enhancements that would have required significant research and implementation time.<br>However, I eventually hit a wall. While Cursor excelled at enhancing existing functionality and adding incremental complexity, it struggled with implementing the more sophisticated algorithms I envisioned. The tool could handle the integration work beautifully, but when it came to translating cutting-edge research concepts into robust, production-ready code, the gap between AI capability and human expertise became apparent.<br>However, it was a different story for a small front-end heavy application. For my_locations project—a map-based UI application—was one of my successful AI-assisted development using Cursor. As someone who readily admits to being weak in frontend code, I was amazed at how the tool guided me through creating a polished, functional map interface. Cursor didn't just generate code; it taught me frontend patterns and best practices that I've since applied to other projects[9][10].<br>When I needed to complete the "How Rich are you?" application with proper backend functionality, Windsurf proved to be the right tool for the job. Its more sophisticated understanding of full-stack architecture and data processing capabilities allowed me to implement the complex financial calculations and data integration that the cloud builders couldn't handle.<br><br>Perhaps the most intriguing development in the AI coding space is GitHub's Spec-Kit framework, which introduces a structured approach to AI-assisted development[11][12]. My most comprehensive exploration of this framework came through developing artmind—my pet "second brain" project that I've iterated through multiple versions over the past months.<br>The initial experience with Spec-Kit combined with Windsurf was genuinely impressive. The framework's systematic approach—moving through Specify, Plan, Tasks, and Implementation phases—created a sense of architectural rigor that ad-hoc prompting lacks[11][13]. For artmind's first iteration, I watched as my high-level vision for a personal knowledge management system expanded into a comprehensive specification covering document processing, semantic search, and q&amp;a.<br>What fascinated me most was how the process became educational. As Spec-Kit generated hundreds of detailed tasks, it introduced me to Python tools and practices I hadn't encountered: ruff for linting, justfile for task automation, alembic for database migrations, and sophisticated pydantic models for data validation. The framework wasn't just generating code; it was exposing me to the broader Python ecosystem and modern development practices.<br>This educational aspect continued as I explored various agent development frameworks during the artmind iterations. I experimented with PydanticAI for structured agent interactions, Atomic for multi-agent coordination, and Instructor for reliable LLM outputs. Each framework revealed its own strengths and limitations—PydanticAI's type safety came with complexity overhead, Atomic's coordination capabilities struggled with state management, and Instructor's reliability sometimes came at the cost of flexibility.<br>However, the fundamental challenge with Spec-Kit became apparent across multiple artmind iterations. The framework's ability to expand a simple specification into hundreds of detailed tasks is genuinely impressive—it incorporates software engineering best practices, accessibility considerations, and robust error handling that individual developers might overlook[11]. But therein lies the problem: it moves so fast that it can easily outpace human comprehension and oversight.<br>By my second artmind iteration, I was dealing with thousands of lines of generated code across dozens of files. Even working feature by feature rather than attempting to build the entire application at once, the sheer volume of generated code became overwhelming. The disconnect between my original vision and the implemented solution grew with each iteration, creating a system that was technically sophisticated but increasingly alien to my understanding.<br><img alt="AI v Human.png" src="blog/ai-v-human.png" style="width: 1200px; max-width: 100%;"><br><br>This overwhelming experience with rapid code generation led me to Warp terminal for my current artmind4 iteration, and the difference in development philosophy has been profound[14][15]. Rather than maximizing code generation speed, Warp focuses on maintaining developer connection to the development process[16][17]. <br>Working on artmind4 through Warp felt fundamentally different from my previous iterations. I was able to use its features to enhance understanding rather than replace it. It provides better responses during error analysis and debugging and also feels like it bites only as much it can chew in one prompt and does more or less what I ask it to[15][18]. Instead of generating massive amounts of code that I struggle to comprehend, Warp took me through incremental development where I understood each step before proceeding.<br>What makes this approach particularly compelling for complex projects like artmind4 is Warp's sequential, terminal-based workflow that naturally enforces a more deliberate pace of development[14]. The tool encourages me to remain engaged with each step of the process, understanding the commands being executed and the changes being made[18]. This creates natural checkpoints where I can evaluate and understand each modification before proceeding—a stark contrast to the overwhelming code dumps from my Spec-Kit experiments.<br>The platform's integration with team knowledge through Warp Drive adds another layer of contextual awareness, allowing AI assistance to be informed by project-specific workflows and documentation[16]. For artmind4, this means the AI understands not just general Python patterns, but the specific architectural decisions and design principles I've established for this iteration.<br><img alt="Tool comparison.png" src="blog/tool-comparison.png" style="width: 600px; max-width: 100%;"><br>Note that different LLMs like Claude Sonnet 4.x, GPT 4.x &amp; 5.x, Gemini/Gemma all have different personalities and have a huge impact on code quality and the SWE workflow. However, that discussion is for another day (or article).<br><br>Recent research reveals a troubling paradox in AI-assisted development that my experience confirms. While usage continues to climb—with 84% of developers now using AI tools—trust in these systems is reducing[19][20]. Stack Overflow's 2025 Developer Survey shows that trust in AI accuracy has fallen from 43% to just 33% over the past year, even as adoption rates continue to rise[19][21]. This decline in trust isn't arbitrary—it's based on real experience with the limitations of current AI systems. The most common frustration, cited by 45% of developers, is dealing with "AI solutions that are almost right, but not quite"[19]. I've encountered this phenomenon repeatedly across my projects. The code appears functional, passes initial tests, but contains subtle logical errors or architectural decisions that create problems down the line[22][23].<br>The importance of maintaining true code ownership in an AI-assisted environment cannot be overstressed. Traditional ownership models assumed that whoever committed code had written and understood it. AI assistance breaks this assumption, creating situations where developers may submit code they don't fully comprehend. My approach is to treat AI suggestions generated code with the duality of enthusiasm and scepticism. Enthusiasm - as I learn new frameworks and clever patterns. But I quickly kill that feeling and apply a skeptic's hat with more scrutiny than I would apply to code from trusted colleagues[24]. For each significant piece of AI-generated code, I ensure I can explain every line, debug any issues that arise, justify the design decisions made, and maintain the code long-term[25]. This means regularly stepping back from the rapid pace of AI generation to truly understand what's being created.<br>During my artmind iterations, I learnt the hard way that accepting AI-generated code without this level of understanding creates technical debt that compounds quickly. I've adopted a rule: if I can't explain why the AI chose a particular approach, I either research it until I understand or request a different implementation that aligns with patterns I comprehend.<br>The legal implications add another layer of complexity that became relevant as my projects grew in sophistication. Current copyright law requires human authorship for protection, meaning code generated primarily by AI may not be eligible for copyright protection[26][27]. For commercial projects, we need to maintain detailed records of the prompts, modifications, and decision-making processes to document human creative input[27][28].<br><br>Through these hands-on examples, I've developed a practical framework for effective AI-assisted development. The human builder should serve as the navigator, making architectural decisions and maintaining strategic oversight, while AI functions as the driver, implementing specific solutions under human guidance[29][30].<br>For the projects that succeeded I provided rich contextual information rather than relying on AI alone to infer requirements and technical decisions. Things worked for a while when I gave the AI an open hand. But I did not let it run ahead of me for too long. I would terminate that branch of code and begin back from a checkpoint where I explicitly shared relevant codebase patterns, architectural decisions, and project constraints[29]. This contextual grounding helped AI generate more appropriate solutions and reduced the likelihood of suggestions that appeared functional but violated established patterns[31].<br>There is therefore the need to embrace iterative development and refinement rather than expecting immediate results. Starting with rough implementations and gradually refining them allows for better oversight and understanding[29][30]. This approach proved crucial during my RAG chatbot enhancements, where each iteration built upon previous understanding rather than attempting massive changes all at once.<br><img alt="Best Practices.png" src="blog/best-practices.png" style="width: 1200px; max-width: 100%;"><br><br>Despite the challenges, the potential for AI tools to accelerate innovation cycles remains one of their most compelling benefits. My vulnerability detection demo exemplifies this perfectly—turning what would have been a month-long development project into a one-hour exercise. The challenge was to create a system that could analyze customer service chat transcripts and identify when agents encountered vulnerable customers—situations requiring special handling protocols. Using Windsurf, I was able to rapidly prototype a solution that incorporated natural language processing and pattern recognition. The speed of development was genuinely remarkable; what would have taken weeks of traditional development was accomplished in a single focused session.<br>The key insight from my experience is that this acceleration is most valuable in specific contexts: proof-of-concept development, rapid prototyping, and demonstrating feasibility to stakeholders. In these scenarios, the ability to quickly materialize ideas into working demonstrations significantly reduced the time between concept and validation[3][35]. This is a genuine benefit and can applied TODAY in all product development scenarios.<br>However, this same acceleration becomes problematic when applied to complex, long-term projects without appropriate guidance and safeguards. The "move fast and break things" mentality that worked for prototypes introduce significant technical debt and maintenance challenges in more sophisticated applications[36][37]. Most AI coding assistants will fail at enterprise scale because they lack understanding of complex, interconnected systems and organizational-specific patterns.<br>During my artmind development, I encountered the challenge of maintaining consistency across multiple AI-generated modules. Generic pattern suggestions from public repository training data sometimes violated the architectural decisions I had established earlier in the project[33][34]. This experience illustrated why enterprises need clear guidelines and standardized approaches to prevent conflicting tools and practices from creating integration challenges. This in fact now becomes a new "job role" of the Architects, Designers or Code Owners within the enterprise - “Encode the standards and patterns into the various SWE Agentic prompts or prompt frameworks themselves”.<br><br>As I continue developing artmind4 and planning future projects, I've established personal principles that guide my use of AI coding tools. These aren't theoretical guidelines—they're practical rules derived from real successes and failures across diverse development challenges.<br>Never let the AI run ahead of your understanding. Whether working with cloud builders for rapid prototyping, serious development environments for production code, or terminal-based tools for controlled development, I maintain my role as architect and steward of the systems I create. The most sophisticated AI tools enhance rather than replace human judgment, creativity, and oversight.<br>The tools will continue to evolve, becoming more powerful and more persuasive in their suggestions. My responsibility as a "builder" is to evolve alongside them, developing the judgment to know when to embrace their assistance and when to assert human control. AI becomes a force multiplier for human creativity when used thoughtfully, but it can quickly become a replacement for human understanding if we're not careful.<br>In this balance between human insight and machine capability lies the path to truly transformative software development—development that harnesses the power of AI without sacrificing the wisdom that only human experience and creativity can provide, and the human oversight which needs to be embedded in. We can unlock faster, richer, and more meaningful systems, tackling previously insurmountable problems. The need of the hour is to foster a culture of adaptability and continuously learn to work and evolve our ways of working with the AI tools. By navigating this transition thoughtfully, we can unlock a future where technology enhances human capabilities and drives sustainable progress&nbsp;and where human ingenuity is amplified, not diminished<br>Stay curious, keep learning, keep creating !!<br>Sources<br>
[1] Vibe coding <a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Vibe_coding" target="_blank">https://en.wikipedia.org/wiki/Vibe_coding</a><br>
[2] What is vibe coding? | AI coding <a rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/learning/ai/ai-vibe-coding/" target="_blank">https://www.cloudflare.com/learning/ai/ai-vibe-coding/</a><br>
[3] Top 10 Vibe Coding AI Tools Every Developer Needs in 2025 <a rel="noopener nofollow" class="external-link" href="https://dev.to/devland/top-10-vibe-coding-ai-tools-every-developer-needs-in-2025-29pf" target="_blank">https://dev.to/devland/top-10-vibe-coding-ai-tools-every-developer-needs-in-2025-29pf</a><br>
[4] Lovable vs. Bolt vs. Bubble: AI Builders Compared <a rel="noopener nofollow" class="external-link" href="https://bubble.io/blog/lovable-vs-bolt-vs-bubble-comparison/" target="_blank">https://bubble.io/blog/lovable-vs-bolt-vs-bubble-comparison/</a><br>
[5] Lovable vs. Bolt: Which AI coding tool is best? [2025] <a rel="noopener nofollow" class="external-link" href="https://zapier.com/blog/lovable-vs-bolt/" target="_blank">https://zapier.com/blog/lovable-vs-bolt/</a><br>
[6] Bolt AI vs Lovable AI: Which AI App Builder Deserves Your ... <a rel="noopener nofollow" class="external-link" href="https://uibakery.io/blog/bolt-ai-vs-lovable-ai" target="_blank">https://uibakery.io/blog/bolt-ai-vs-lovable-ai</a><br>
[7] Cursor vs Windsurf: An In-Depth Comparison <a rel="noopener nofollow" class="external-link" href="https://www.appypievibe.ai/blog/cursor-vs-windsurf-ai-code-editor" target="_blank">https://www.appypievibe.ai/blog/cursor-vs-windsurf-ai-code-editor</a><br>
[8] Cursor vs Windsurf: Full AI Coding Tool Comparison <a rel="noopener nofollow" class="external-link" href="https://www.sevensquaretech.com/windsurf-vs-cursor-features-comparison-coding/" target="_blank">https://www.sevensquaretech.com/windsurf-vs-cursor-features-comparison-coding/</a><br>
[9] Cursor vs Windsurf: AI Coding Assistant Comparison <a rel="noopener nofollow" class="external-link" href="https://www.tembo.io/blog/cursor-vs-windsurf" target="_blank">https://www.tembo.io/blog/cursor-vs-windsurf</a><br>
[10] Windsurf vs Cursor: Which AI IDE Tool is Better? <a rel="noopener nofollow" class="external-link" href="https://www.qodo.ai/blog/windsurf-vs-cursor/" target="_blank">https://www.qodo.ai/blog/windsurf-vs-cursor/</a><br>
[11] GitHub Spec Kit: A Guide to Spec-Driven AI Development <a rel="noopener nofollow" class="external-link" href="https://intuitionlabs.ai/articles/spec-driven-development-spec-kit" target="_blank">https://intuitionlabs.ai/articles/spec-driven-development-spec-kit</a><br>
[12] A look at Spec Kit, GitHub's spec-driven software ... <a rel="noopener nofollow" class="external-link" href="https://ainativedev.io/news/a-look-at-spec-kit-githubs-spec-driven-software-development-toolkit" target="_blank">https://ainativedev.io/news/a-look-at-spec-kit-githubs-spec-driven-software-development-toolkit</a><br>
[13] Diving Into Spec-Driven Development With GitHub Spec Kit <a rel="noopener nofollow" class="external-link" href="https://developer.microsoft.com/blog/spec-driven-development-spec-kit" target="_blank">https://developer.microsoft.com/blog/spec-driven-development-spec-kit</a><br>
[14] Warp Wrapped: 2024 in Review <a rel="noopener nofollow" class="external-link" href="https://www.warp.dev/blog/2024-in-review" target="_blank">https://www.warp.dev/blog/2024-in-review</a><br>
[15] Warp: a new AI-based terminal you should try out <a rel="noopener nofollow" class="external-link" href="https://daily.dev/blog/warp-a-new-ai-based-terminal-you-should-try-out" target="_blank">https://daily.dev/blog/warp-a-new-ai-based-terminal-you-should-try-out</a><br>
[16] All Features <a rel="noopener nofollow" class="external-link" href="https://www.warp.dev/all-features" target="_blank">https://www.warp.dev/all-features</a><br>
[17] Warp: AI: Natural‑Language Coding Agents <a rel="noopener nofollow" class="external-link" href="https://www.warp.dev/warp-ai" target="_blank">https://www.warp.dev/warp-ai</a><br>
[18] Warp AI Terminal: A Beginner's Guide to the Future of ... <a rel="noopener nofollow" class="external-link" href="https://dev.to/arjun98k/warp-ai-terminal-a-beginners-guide-to-the-future-of-command-line-interfaces-43k1" target="_blank">https://dev.to/arjun98k/warp-ai-terminal-a-beginners-guide-to-the-future-of-command-line-interfaces-43k1</a><br>
[19] Developers remain willing but reluctant to use AI: The 2025 ... <a rel="noopener nofollow" class="external-link" href="https://stackoverflow.blog/2025/07/29/developers-remain-willing-but-reluctant-to-use-ai-the-2025-developer-survey-results-are-here/" target="_blank">https://stackoverflow.blog/2025/07/29/developers-remain-willing-but-reluctant-to-use-ai-the-2025-developer-survey-results-are-here/</a><br>
[20] Trends #8: Developers use AI more, but they trust it much less <a rel="noopener nofollow" class="external-link" href="https://newsletter.techworld-with-milan.com/p/trends-8-developers-use-ai-more-but" target="_blank">https://newsletter.techworld-with-milan.com/p/trends-8-developers-use-ai-more-but</a><br>
[21] Most developers use AI in their daily workflows - but they ... <a rel="noopener nofollow" class="external-link" href="https://www.zdnet.com/article/most-developers-use-ai-daily-in-their-workflows-but-they-dont-trust-it-study-finds/" target="_blank">https://www.zdnet.com/article/most-developers-use-ai-daily-in-their-workflows-but-they-dont-trust-it-study-finds/</a><br>
[22] The Productivity Paradox of AI Coding Assistants <a rel="noopener nofollow" class="external-link" href="https://www.cerbos.dev/blog/productivity-paradox-of-ai-coding-assistants" target="_blank">https://www.cerbos.dev/blog/productivity-paradox-of-ai-coding-assistants</a><br>
[23] Trust in AI coding tools is plummeting <a rel="noopener nofollow" class="external-link" href="https://leaddev.com/technical-direction/trust-in-ai-coding-tools-is-plummeting" target="_blank">https://leaddev.com/technical-direction/trust-in-ai-coding-tools-is-plummeting</a><br>
[24] How Do I Maintain Code Ownership When Using AI? <a rel="noopener nofollow" class="external-link" href="https://zenvanriel.nl/ai-engineer-blog/how-do-i-maintain-code-ownership-when-using-ai/" target="_blank">https://zenvanriel.nl/ai-engineer-blog/how-do-i-maintain-code-ownership-when-using-ai/</a><br>
[25] Maintaining Code Ownership in the Age of AI Assistance <a rel="noopener nofollow" class="external-link" href="https://zenvanriel.nl/ai-engineer-blog/maintaining-code-ownership-with-ai-assistance/" target="_blank">https://zenvanriel.nl/ai-engineer-blog/maintaining-code-ownership-with-ai-assistance/</a><br>
[26] Navigating the Legal Landscape of AI-Generated Code <a rel="noopener nofollow" class="external-link" href="https://www.mbhb.com/intelligence/snippets/navigating-the-legal-landscape-of-ai-generated-code-ownership-and-liability-challenges/" target="_blank">https://www.mbhb.com/intelligence/snippets/navigating-the-legal-landscape-of-ai-generated-code-ownership-and-liability-challenges/</a><br>
[27] Think While You Are Using AI Coding <a rel="noopener nofollow" class="external-link" href="https://www.bakerdonelson.com/think-while-you-are-using-ai-coding" target="_blank">https://www.bakerdonelson.com/think-while-you-are-using-ai-coding</a><br>
[28] AI-Generated Code: Who Owns the Intellectual Property ... <a rel="noopener nofollow" class="external-link" href="https://www.leadrpro.com/blog/who-really-owns-code-when-ai-does-the-writing" target="_blank">https://www.leadrpro.com/blog/who-really-owns-code-when-ai-does-the-writing</a><br>
[29] Best practices for pair programming with AI assistants <a rel="noopener nofollow" class="external-link" href="https://graphite.dev/guides/ai-pair-programming-best-practices" target="_blank">https://graphite.dev/guides/ai-pair-programming-best-practices</a><br>
[30] Pair Programming with AI Coding Agents: Is It Beneficial? <a rel="noopener nofollow" class="external-link" href="https://zencoder.ai/blog/best-practices-for-pair-programming-with-ai-coding-agents" target="_blank">https://zencoder.ai/blog/best-practices-for-pair-programming-with-ai-coding-agents</a><br>
[31] AI-Powered Coding Assistants: Best Practices to Boost ... <a rel="noopener nofollow" class="external-link" href="https://www.monterail.com/blog/ai-powered-coding-assistants-best-practices" target="_blank">https://www.monterail.com/blog/ai-powered-coding-assistants-best-practices</a><br>
[32] AI Coding Assistant for Enterprises: Beyond GitHub Copilot <a rel="noopener nofollow" class="external-link" href="https://fx31labs.com/ai-coding-assistant-enterprise-tools/" target="_blank">https://fx31labs.com/ai-coding-assistant-enterprise-tools/</a><br>
[33] AI Coding Assistants for Large Codebases: A Complete ... <a rel="noopener nofollow" class="external-link" href="https://www.augmentcode.com/guides/ai-coding-assistants-for-large-codebases-a-complete-guide" target="_blank">https://www.augmentcode.com/guides/ai-coding-assistants-for-large-codebases-a-complete-guide</a><br>
[34] AI Coding Tools Are Not Enough. Here's What Enterprises ... <a rel="noopener nofollow" class="external-link" href="https://www.codespell.ai/blog/ai-coding-tools-are-not-enough-heres-what-enterprises-actually-need" target="_blank">https://www.codespell.ai/blog/ai-coding-tools-are-not-enough-heres-what-enterprises-actually-need</a><br>
[35] Best Vibe Coding Tools &amp; Why AI Agents Work Better with ... <a rel="noopener nofollow" class="external-link" href="https://codehooks.io/blog/vibe-coding-tools" target="_blank">https://codehooks.io/blog/vibe-coding-tools</a><br>
[36] The Productivity Trap <a rel="noopener nofollow" class="external-link" href="https://matthewreinbold.com/2025/06/19/theproductivitytrap" target="_blank">https://matthewreinbold.com/2025/06/19/theproductivitytrap</a><br>
[37] Leading Developer Productivity Tools in 2025 <a rel="noopener nofollow" class="external-link" href="https://www.codeant.ai/blogs/leading-developer-productivity-tools" target="_blank">https://www.codeant.ai/blogs/leading-developer-productivity-tools</a><br>
[38] Limitations of AI Coding Assistants: What You Need to Know <a rel="noopener nofollow" class="external-link" href="https://zencoder.ai/blog/limitations-of-ai-coding-assistants" target="_blank">https://zencoder.ai/blog/limitations-of-ai-coding-assistants</a>]]></description><link>blog/don't-let-the-ai-run-ahead-of-yourself.html</link><guid isPermaLink="false">Blog/Don't let the AI run ahead of yourself.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 12 Oct 2025 17:08:15 GMT</pubDate><enclosure url="blog/don't-let-the-ai-run-ahead-of-yourself-2.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="blog/don't-let-the-ai-run-ahead-of-yourself-2.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Finding your authentic self]]></title><description><![CDATA[<a class="tag" href="?query=tag:reflection" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#reflection</a> 
 <br>header<br><br><br><br><a href=".?query=tag:reflection" class="tag" target="_blank" rel="noopener nofollow">#reflection</a><br>
Life is supposed to be beautiful, like a grand landscape of nature. But why do you feel that instead of enjoying the grandeur, you are instead, down in the rapids, being buffeted by the strong currents. Unable to be in control of the series of competing endeavours, due to the fear of losing out. Driven in all directions by micro but perceptually powerful forces. If you don’t find your anchor you will be swept away.<br>You need to find your anchor, your authentic self. The forces will still come at you, but you will be capable of allowing the rapids flow past. And start to enjoy the experience and see the bigger picture, the grand landscape.<br>Now you might think that finding your authentic self should be easy. After all, it is about you. But it takes a good bit of effort and introspection. It is an ongoing process of self discovery. I am somewhere along the process too.<br>I have realised my anchor is in knowledge and constant learning keeps me moored. Artificial Intelligence (AI), Societal cause in the form of human-ness and Art are the areas I am exploring. This blog will therefore be a log of those small steps, and signposts along this journey of exploration, and my attempt to spread the knowledge I gain. One day I hope to write books and lecture extensively, but that’s for another phase.]]></description><link>blog/finding-your-authentic-self.html</link><guid isPermaLink="false">Blog/Finding your authentic self.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 20 Jul 2025 05:36:03 GMT</pubDate></item><item><title><![CDATA[While sleeping your brain becomes a ML machine]]></title><description><![CDATA[ 
 <br>header<br><br><br><br>Have you ever wondered why your brain feels so remarkably refreshed after a good night's sleep? Or why breakthrough insights often emerge after "sleeping on it"? The answer lies in one of nature's most elegant computational processes: your sleeping brain operates as a sophisticated machine learning system, continuously optimizing its neural networks through the seemingly chaotic experience we call dreams.<br><img alt="Brains do Machine Learning.png" src="blog/brains-do-machine-learning.png"><br>Recent advances in neuroscience and machine learning have revealed striking parallels between the brain's nocturnal activities and artificial neural network optimization processes. Did you know that while you sleep, your brain is essentially running gradient descent algorithms to fine-tune the connections that define your thoughts, memories, and behaviors? This convergence of evidence suggests that sleep functions as a biological machine learning system, with dreams serving as the experiential manifestation of neural optimization processes.<br>The article covers:<br>
<br>Neural Architecture of Sleep&nbsp;- How sleep stages mirror neural network training phases
<br>Dreams as Gradient Descent&nbsp;- Evidence that dreaming about learning tasks is associated with performance improvement
<br>Memory Consolidation as Training&nbsp;- How sleep consolidates fragile memory traces into permanent storage
<br>Emotional Processing&nbsp;- How dream affect modulates memory consolidation processes
<br>Neural Housekeeping&nbsp;- The brain's equivalent of neural network regularization
<br>Consciousness Implications&nbsp;- How gradient descent learning creates efficient neural representations
<br><br>Sleep is characterized by distinct stages, each serving specific functions in memory consolidation and neural optimization. REM sleep, NREM sleep, and the N2 transition to REM (characterized by sleep spindles) are integral to memory consolidation. Each stage serves a specific computational purpose, much like the different phases of training an artificial neural network.<br>During waking hours, your brain accumulates vast amounts of sensory and experiential data—think of this as the "training dataset" for your neural networks. But here's where it gets fascinating: sleep provides the computational window for processing this information. We consider the formation of long-term memory during sleep as an active systems consolidation process that is embedded in a process of global synaptic downscaling. Isn't it remarkable that your brain has evolved its own version of regularization techniques to prevent overfitting?<br>Neuroscientific theories suggest that dreams result from an interplay between top-down (abstract, knowledge-driven) and bottom-up (sensory-driven) brain processes. During sleep, reduced external input allows internal associative networks to spontaneously activate, producing novel combinations of memory fragments and percepts—a process paralleling how ANNs generate synthetic data when retraining with noise for better generalization.<br><br>Why do dreams feel so random and chaotic? The answer might surprise you. Dreams can be conceptualized as the experiential byproduct of gradient descent optimization occurring in neural networks. Gradient descent is an optimization algorithm used to train machine learning models by minimizing errors between predicted and actual results. Your sleeping brain is doing exactly this—minimizing prediction errors and optimizing neural pathways based on the day's experiences.<br>Consider this: in artificial neural networks, gradient descent algorithms introduce randomness to avoid local minima and explore the solution space more effectively. Sound familiar? Dreams present fragmented, recombined memories in novel configurations, allowing your brain to explore different neural pathways and optimize connections in ways that pure logical reasoning cannot achieve. The seemingly nonsensical narrative of dreams isn't a bug—it's a feature of the optimization process.<br>Why can't we simply think our way to optimal solutions? The answer lies in the limitations of deterministic processing. Methods from convex optimization such as accelerated gradient descent are widely used as building blocks for deep learning algorithms. Your brain simply employs the stochastic nature of neural exploration during sleep. Therefor, the randomness you experience in dreams isn't meaningless noise—it's the stochastic exploration necessary for effective neural optimization. Just as machine learning algorithms use random sampling to escape local optima and find global solutions, your dreaming brain explores unlikely combinations of memories and concepts. This is why breakthrough insights often emerge after sleep: your brain has literally explored solution spaces that conscious reasoning couldn't access.<br><br>Have you ever wondered why some memories stick while others fade? Sleep enhances memory consolidation, especially for complex declarative information. This process mirrors the training protocols used in machine learning systems, where important patterns are reinforced while noise is filtered out.<br>But here's what's truly fascinating: In the awake brain, information about the external world reaches the hippocampus via the entorhinal cortex, whereas during sleep there is also a predominant reverse direction of information flow: population bursts initiated in the hippocampus invade the neocortex. This architectural shift allows for the systematic transfer of information from temporary storage to permanent neural networks.<br><br>Why do emotionally charged dreams feel so vivid and impactful? Dreams often carry strong emotional content, which appears to play a crucial role in the optimization process. Your brain's emotional evaluation system during sleep functions as a sophisticated loss function, determining which memories deserve strengthening and which should be weakened or discarded.<br>This emotional weighting system resembles the attention mechanisms used in modern neural networks, where certain inputs receive higher priority during processing. Negative emotional experiences in dreams may signal important learning opportunities that require additional neural resources for optimal encoding. Have you noticed how emotionally significant events from your day often reappear in dreams? This isn't coincidence—it's your brain's optimization algorithm at work.<br><br>A lot of the concepts discussed above from the biological function of "sleep" perspective is posited in a scientific hypothesis called the Synaptic Homeostasis hYpothesis (SHY). SHY posits that wakefulness is dominated by synaptic potentiation—connections between neurons strengthen as we learn and interact with the world. Sleep, particularly deep NREM and REM sleep, is when the brain “downscales” or prunes unnecessary synaptic connections. This neural housekeeping declutters circuits, maintains energy efficiency, and ensures only the most relevant pathways are retained for future cognitive processing.<br>There is empirical evidence from mouse studies that show REM sleep selectively prunes new synapses formed during waking experiences. Mice deprived of REM retain more irrelevant synaptic connections and have impaired memory consolidation. Human research demonstrates that dream content is loosely related to daily experiences; dreams rarely replay episodes but instead mix memory fragments in novel ways, potentially aiding creative problem-solving and insight.<br>
There are real cognitive benefits in the form of enhanced learning and memory. Pruning strengthens frequently used pathways and discards weak, irrelevant ones, much like optimizing a neural network for accuracy and efficiency. This prevents cognitive overload by reducing synaptic “noise,” the brain increases clarity, problem-solving capacity, and creativity. Also a streamlined neural network reduces metabolic demands on the brain and helps in energy conservation.<br>The brain’s synaptic pruning during sleep, especially as explained in the Synaptic Homeostasis Hypothesis (SHY), has striking parallels in Deep Learning and Machine Learning. Here’s how modern techniques in AI mimic these processes:<br><br><br>Here's a mind-bending realization: consciousness isn't a fixed state but rather emerges from an ever-evolving neural network that continuously updates its weights and connections every single night. For decades, it has been demonstrated that sleep plays an important role in long-term memory consolidation. This means that who you are today is literally different from who you were yesterday, thanks to the neural optimization that occurred during sleep.<br>The continuous nature of this optimization process—occurring every night throughout your life—challenges traditional views of consciousness as a static phenomenon. Instead, it presents consciousness as a dynamic, continuously optimized system. Isn't it remarkable that you're essentially a biological neural network that never stops learning, even unconsciously?<br><br>Understanding sleep as a machine learning system opens fascinating possibilities. Could studying sleep disorders provide insights into neural network optimization failures? Might advances in machine learning optimization techniques inform treatments for sleep-related cognitive impairments? These questions are at the forefront of current research.<br>The integration of sleep research with machine learning principles also has implications for developing more efficient artificial neural networks. By understanding how biological systems optimize during downtime, we might develop better training procedures and maintenance protocols for artificial systems. Isn't it intriguing that the solution to better AI might lie in understanding why we sleep?<br>So the next time you drift off to sleep, remember: you're not just resting—you're actively optimizing the neural networks that define your conscious experience. Sweet dreams, and may your gradient descent find optimal solutions.]]></description><link>blog/while-sleeping-our-brains-become-an-ml-machine.html</link><guid isPermaLink="false">Blog/While sleeping our brains become an ML machine.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 20 Jul 2025 09:09:32 GMT</pubDate><enclosure url="blog/brains-do-machine-learning.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="blog/brains-do-machine-learning.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Architectural Overview]]></title><description><![CDATA[ 
 <br><a data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow">Index</a><br><br>Loren impsum<br>]]></description><link>product_backlog/architecture.html</link><guid isPermaLink="false">Product_Backlog/Architecture.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 10 Dec 2023 14:01:44 GMT</pubDate></item><item><title><![CDATA[Core Concepts]]></title><description><![CDATA[ 
 <br><a data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow">Index</a><br><br><br>This is the main areas of "research". Use reasoning for all kinds of tasks:<br>
<br>Meta reasoning or the "Director" for switching contexts
<br>To figure out how to learn new skills
<br><br>Consists of three sub-domains:<br><br>This is the main knowledge base of the artifex. Conceptualized as a Graph database.<br><br>This is in the form of knowledge shards in a vector database. This would be populated by uploaded or input documents coming in from the human inputs<br><br>Derived knowledge from the other types of knowledge. Some amount of introverted thinking. More than just facts. Maybe values, course corrections, meta knowledge<br><br>Consists of two sub-domains:<br><br>Coded by the human<br><br>Skills that could be taught (and code generated) via human chat inputs, wisdom and artiflex reasoning<br><br>Retrieval of information, answering questions including calculations, processing and reasoning]]></description><link>product_backlog/core-concepts.html</link><guid isPermaLink="false">Product_Backlog/Core Concepts.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 17 Mar 2024 06:19:19 GMT</pubDate></item><item><title><![CDATA[How did we go about building it]]></title><description><![CDATA[ 
 <br><a data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow">Index</a><br><br><br>
<br>9th Dec: 

<br>Created the base streamlit chat app, and made calls to OpenAI GPT models. 
<br>Took care of chat history and logging
<br>Used OpenAI's GPT 3.5 &amp; 4 models to get the base code for individual functionality. 

<br>But there were errors - it could not code the chat history in streamlit. It did not understand the st.session construct
<br>And it could not write code for loguru - different levels to different channels or files. For that I had to go back and research the web and find answers




<br>10th Dec:

<br>Found something very interesting in this <a data-tooltip-position="top" aria-label="https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a" rel="noopener nofollow" class="external-link" href="https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a" target="_blank">article</a>

<br>ollama - to run models locally
<br>zephyr - a powerful open source model


<br>I installed ollama and played with it today


]]></description><link>product_backlog/how-did-we-go-about-building-it.html</link><guid isPermaLink="false">Product_Backlog/How did we go about building it.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sat, 28 Jun 2025 13:09:00 GMT</pubDate></item><item><title><![CDATA[header]]></title><description><![CDATA[ 
 <br>]]></description><link>header.html</link><guid isPermaLink="false">header.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 29 Jun 2025 05:18:23 GMT</pubDate></item><item><title><![CDATA[Index]]></title><description><![CDATA[<a class="tag" href="?query=tag:technology" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#technology</a> <a class="tag" href="?query=tag:inspiration" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#inspiration</a> <a class="tag" href="?query=tag:technology" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#technology</a> 
 <br>header<br><br><br>
<img alt="attachments/NeuronGraph_Logo_2.png" src="attachments/neurongraph_logo_2.png"><br>
<img alt="attachments/NeuroGraphical_Landscape.jpg" src="attachments/neurographical_landscape.jpg"><br>This is NeuronGraph. A blog focusing on :<br>
ART . TECHNOLOGY . PHILOSOPHY<br>
Visionating, Innovating and creating for the convergence of Art, Technology and Society. <br><br>प्रतिबोधविदितं मतं हि अमृतत्वं विन्दते। आत्मना वीर्यं विन्दते। विद्यया अमृतं विन्दते ॥ <a data-tooltip-position="top" aria-label="https://upanishads.org.in/upanishads/2/2/4" rel="noopener nofollow" class="external-link" href="https://upanishads.org.in/upanishads/2/2/4" target="_blank">from</a><br>
When It is known by perception that reflects It, then one has the thought of It, for one finds immortality; by the self one finds the force to attain and by the knowledge one finds immortality.<br><br>]]></description><link>index.html</link><guid isPermaLink="false">Index.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 19 Oct 2025 05:23:45 GMT</pubDate><enclosure url="attachments/neurongraph_logo_2.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="attachments/neurongraph_logo_2.png"&gt;&lt;/figure&gt;</content:encoded></item></channel></rss>