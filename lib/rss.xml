<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[NeuronGraph_website]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib/media/favicon.png</url><title>NeuronGraph_website</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Sun, 20 Jul 2025 07:44:09 GMT</lastBuildDate><atom:link href="lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Sun, 20 Jul 2025 07:44:09 GMT</pubDate><copyright><![CDATA[Surjit Das]]></copyright><ttl>60</ttl><dc:creator>Surjit Das</dc:creator><item><title><![CDATA[About]]></title><description><![CDATA[ 
 <br>header<br><br><br>Please get in touch with me via any of my social media handles]]></description><link>blog/about.html</link><guid isPermaLink="false">Blog/About.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 20 Jul 2025 05:36:03 GMT</pubDate></item><item><title><![CDATA[2025]]></title><description><![CDATA[<a class="tag" href="?query=tag:reflection" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#reflection</a> <a class="tag" href="?query=tag:technology" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#technology</a> <a class="tag" href="?query=tag:inspiration" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#inspiration</a> 
 <br>header<br><br><br>List of all the blog entries:<br><br><br><br>]]></description><link>blog/all-writings.html</link><guid isPermaLink="false">Blog/All Writings.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 20 Jul 2025 07:42:01 GMT</pubDate></item><item><title><![CDATA[Finding your authentic self]]></title><description><![CDATA[<a class="tag" href="?query=tag:reflection" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#reflection</a> 
 <br>header<br><br><br><br><a href=".?query=tag:reflection" class="tag" target="_blank" rel="noopener nofollow">#reflection</a><br>
Life is supposed to be beautiful, like a grand landscape of nature. But why do you feel that instead of enjoying the grandeur, you are instead, down in the rapids, being buffeted by the strong currents. Unable to be in control of the series of competing endeavours, due to the fear of losing out. Driven in all directions by micro but perceptually powerful forces. If you don’t find your anchor you will be swept away.<br>You need to find your anchor, your authentic self. The forces will still come at you, but you will be capable of allowing the rapids flow past. And start to enjoy the experience and see the bigger picture, the grand landscape.<br>Now you might think that finding your authentic self should be easy. After all, it is about you. But it takes a good bit of effort and introspection. It is an ongoing process of self discovery. I am somewhere along the process too.<br>I have realised my anchor is in knowledge and constant learning keeps me moored. Artificial Intelligence (AI), Societal cause in the form of human-ness and Art are the areas I am exploring. This blog will therefore be a log of those small steps, and signposts along this journey of exploration, and my attempt to spread the knowledge I gain. One day I hope to write books and lecture extensively, but that’s for another phase.]]></description><link>blog/finding-your-authentic-self.html</link><guid isPermaLink="false">Blog/Finding your authentic self.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 20 Jul 2025 05:36:03 GMT</pubDate></item><item><title><![CDATA[While sleeping your brain becomes a ML machine]]></title><description><![CDATA[ 
 <br>header<br><br><br><br>Have you ever wondered why your brain feels so remarkably refreshed after a good night's sleep? Or why breakthrough insights often emerge after "sleeping on it"? The answer lies in one of nature's most elegant computational processes: your sleeping brain operates as a sophisticated machine learning system, continuously optimizing its neural networks through the seemingly chaotic experience we call dreams.<br>Recent advances in neuroscience and machine learning have revealed striking parallels between the brain's nocturnal activities and artificial neural network optimization processes. Did you know that while you sleep, your brain is essentially running gradient descent algorithms to fine-tune the connections that define your thoughts, memories, and behaviors? This convergence of evidence suggests that sleep functions as a biological machine learning system, with dreams serving as the experiential manifestation of neural optimization processes.<br>The article covers:<br>
<br>Neural Architecture of Sleep&nbsp;- How sleep stages mirror neural network training phases
<br>Dreams as Gradient Descent&nbsp;- Evidence that dreaming about learning tasks is associated with performance improvement
<br>Memory Consolidation as Training&nbsp;- How sleep consolidates fragile memory traces into permanent storage
<br>Emotional Processing&nbsp;- How dream affect modulates memory consolidation processes
<br>Neural Housekeeping&nbsp;- The brain's equivalent of neural network regularization
<br>Consciousness Implications&nbsp;- How gradient descent learning creates efficient neural representations
<br><br>Sleep is characterized by distinct stages, each serving specific functions in memory consolidation and neural optimization. REM sleep, NREM sleep, and the N2 transition to REM (characterized by sleep spindles) are integral to memory consolidation. Each stage serves a specific computational purpose, much like the different phases of training an artificial neural network.<br>During waking hours, your brain accumulates vast amounts of sensory and experiential data—think of this as the "training dataset" for your neural networks. But here's where it gets fascinating: sleep provides the computational window for processing this information. We consider the formation of long-term memory during sleep as an active systems consolidation process that is embedded in a process of global synaptic downscaling. Isn't it remarkable that your brain has evolved its own version of regularization techniques to prevent overfitting?<br>Neuroscientific theories suggest that dreams result from an interplay between top-down (abstract, knowledge-driven) and bottom-up (sensory-driven) brain processes. During sleep, reduced external input allows internal associative networks to spontaneously activate, producing novel combinations of memory fragments and percepts—a process paralleling how ANNs generate synthetic data when retraining with noise for better generalization.<br><br>Why do dreams feel so random and chaotic? The answer might surprise you. Dreams can be conceptualized as the experiential byproduct of gradient descent optimization occurring in neural networks. Gradient descent is an optimization algorithm used to train machine learning models by minimizing errors between predicted and actual results. Your sleeping brain is doing exactly this—minimizing prediction errors and optimizing neural pathways based on the day's experiences.<br>Consider this: in artificial neural networks, gradient descent algorithms introduce randomness to avoid local minima and explore the solution space more effectively. Sound familiar? Dreams present fragmented, recombined memories in novel configurations, allowing your brain to explore different neural pathways and optimize connections in ways that pure logical reasoning cannot achieve. The seemingly nonsensical narrative of dreams isn't a bug—it's a feature of the optimization process.<br>Why can't we simply think our way to optimal solutions? The answer lies in the limitations of deterministic processing. Methods from convex optimization such as accelerated gradient descent are widely used as building blocks for deep learning algorithms. Your brain simply employs the stochastic nature of neural exploration during sleep. Therefor, the randomness you experience in dreams isn't meaningless noise—it's the stochastic exploration necessary for effective neural optimization. Just as machine learning algorithms use random sampling to escape local optima and find global solutions, your dreaming brain explores unlikely combinations of memories and concepts. This is why breakthrough insights often emerge after sleep: your brain has literally explored solution spaces that conscious reasoning couldn't access.<br><br>Have you ever wondered why some memories stick while others fade? Sleep enhances memory consolidation, especially for complex declarative information. This process mirrors the training protocols used in machine learning systems, where important patterns are reinforced while noise is filtered out.<br>But here's what's truly fascinating: In the awake brain, information about the external world reaches the hippocampus via the entorhinal cortex, whereas during sleep there is also a predominant reverse direction of information flow: population bursts initiated in the hippocampus invade the neocortex. This architectural shift allows for the systematic transfer of information from temporary storage to permanent neural networks.<br><br>Why do emotionally charged dreams feel so vivid and impactful? Dreams often carry strong emotional content, which appears to play a crucial role in the optimization process. Your brain's emotional evaluation system during sleep functions as a sophisticated loss function, determining which memories deserve strengthening and which should be weakened or discarded.<br>This emotional weighting system resembles the attention mechanisms used in modern neural networks, where certain inputs receive higher priority during processing. Negative emotional experiences in dreams may signal important learning opportunities that require additional neural resources for optimal encoding. Have you noticed how emotionally significant events from your day often reappear in dreams? This isn't coincidence—it's your brain's optimization algorithm at work.<br><br>A lot of the concepts discussed above from the biological function of "sleep" perspective is posited in a scientific hypothesis called the Synaptic Homeostasis hYpothesis (SHY). SHY posits that wakefulness is dominated by synaptic potentiation—connections between neurons strengthen as we learn and interact with the world. Sleep, particularly deep NREM and REM sleep, is when the brain “downscales” or prunes unnecessary synaptic connections. This neural housekeeping declutters circuits, maintains energy efficiency, and ensures only the most relevant pathways are retained for future cognitive processing.<br>There is empirical evidence from mouse studies that show REM sleep selectively prunes new synapses formed during waking experiences. Mice deprived of REM retain more irrelevant synaptic connections and have impaired memory consolidation. Human research demonstrates that dream content is loosely related to daily experiences; dreams rarely replay episodes but instead mix memory fragments in novel ways, potentially aiding creative problem-solving and insight.<br>
There are real cognitive benefits in the form of enhanced learning and memory. Pruning strengthens frequently used pathways and discards weak, irrelevant ones, much like optimizing a neural network for accuracy and efficiency. This prevents cognitive overload by reducing synaptic “noise,” the brain increases clarity, problem-solving capacity, and creativity. Also a streamlined neural network reduces metabolic demands on the brain and helps in energy conservation.<br>The brain’s synaptic pruning during sleep, especially as explained in the Synaptic Homeostasis Hypothesis (SHY), has striking parallels in Deep Learning and Machine Learning. Here’s how modern techniques in AI mimic these processes:<br><br><br>Here's a mind-bending realization: consciousness isn't a fixed state but rather emerges from an ever-evolving neural network that continuously updates its weights and connections every single night. For decades, it has been demonstrated that sleep plays an important role in long-term memory consolidation. This means that who you are today is literally different from who you were yesterday, thanks to the neural optimization that occurred during sleep.<br>The continuous nature of this optimization process—occurring every night throughout your life—challenges traditional views of consciousness as a static phenomenon. Instead, it presents consciousness as a dynamic, continuously optimized system. Isn't it remarkable that you're essentially a biological neural network that never stops learning, even unconsciously?<br><br>Understanding sleep as a machine learning system opens fascinating possibilities. Could studying sleep disorders provide insights into neural network optimization failures? Might advances in machine learning optimization techniques inform treatments for sleep-related cognitive impairments? These questions are at the forefront of current research.<br>The integration of sleep research with machine learning principles also has implications for developing more efficient artificial neural networks. By understanding how biological systems optimize during downtime, we might develop better training procedures and maintenance protocols for artificial systems. Isn't it intriguing that the solution to better AI might lie in understanding why we sleep?<br>So the next time you drift off to sleep, remember: you're not just resting—you're actively optimizing the neural networks that define your conscious experience. Sweet dreams, and may your gradient descent find optimal solutions.]]></description><link>blog/while-sleeping-our-brains-become-an-ml-machine.html</link><guid isPermaLink="false">Blog/While sleeping our brains become an ML machine.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 20 Jul 2025 07:36:02 GMT</pubDate></item><item><title><![CDATA[Architectural Overview]]></title><description><![CDATA[ 
 <br><a data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow">Index</a><br><br>Loren impsum<br>]]></description><link>product_backlog/architecture.html</link><guid isPermaLink="false">Product_Backlog/Architecture.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 10 Dec 2023 14:01:44 GMT</pubDate></item><item><title><![CDATA[Core Concepts]]></title><description><![CDATA[ 
 <br><a data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow">Index</a><br><br><br>This is the main areas of "research". Use reasoning for all kinds of tasks:<br>
<br>Meta reasoning or the "Director" for switching contexts
<br>To figure out how to learn new skills
<br><br>Consists of three sub-domains:<br><br>This is the main knowledge base of the artifex. Conceptualized as a Graph database.<br><br>This is in the form of knowledge shards in a vector database. This would be populated by uploaded or input documents coming in from the human inputs<br><br>Derived knowledge from the other types of knowledge. Some amount of introverted thinking. More than just facts. Maybe values, course corrections, meta knowledge<br><br>Consists of two sub-domains:<br><br>Coded by the human<br><br>Skills that could be taught (and code generated) via human chat inputs, wisdom and artiflex reasoning<br><br>Retrieval of information, answering questions including calculations, processing and reasoning]]></description><link>product_backlog/core-concepts.html</link><guid isPermaLink="false">Product_Backlog/Core Concepts.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 17 Mar 2024 06:19:19 GMT</pubDate></item><item><title><![CDATA[How did we go about building it]]></title><description><![CDATA[ 
 <br><a data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow">Index</a><br><br><br>
<br>9th Dec: 

<br>Created the base streamlit chat app, and made calls to OpenAI GPT models. 
<br>Took care of chat history and logging
<br>Used OpenAI's GPT 3.5 &amp; 4 models to get the base code for individual functionality. 

<br>But there were errors - it could not code the chat history in streamlit. It did not understand the st.session construct
<br>And it could not write code for loguru - different levels to different channels or files. For that I had to go back and research the web and find answers




<br>10th Dec:

<br>Found something very interesting in this <a data-tooltip-position="top" aria-label="https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a" rel="noopener nofollow" class="external-link" href="https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a" target="_blank">article</a>

<br>ollama - to run models locally
<br>zephyr - a powerful open source model


<br>I installed ollama and played with it today


]]></description><link>product_backlog/how-did-we-go-about-building-it.html</link><guid isPermaLink="false">Product_Backlog/How did we go about building it.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sat, 28 Jun 2025 13:09:00 GMT</pubDate></item><item><title><![CDATA[header]]></title><description><![CDATA[ 
 <br>]]></description><link>header.html</link><guid isPermaLink="false">header.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 29 Jun 2025 05:18:23 GMT</pubDate></item><item><title><![CDATA[Index]]></title><description><![CDATA[<a class="tag" href="?query=tag:reflection" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#reflection</a> <a class="tag" href="?query=tag:technology" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#technology</a> <a class="tag" href="?query=tag:inspiration" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#inspiration</a> 
 <br>header<br><br><br>
<img alt="attachments/NeuronGraph_Logo_2.png" src="attachments/neurongraph_logo_2.png"><br>
<img alt="attachments/NeuroGraphical_Landscape.jpg" src="attachments/neurographical_landscape.jpg"><br>This is NeuronGraph. A blog focusing on :<br>
ART . TECHNOLOGY . PHILOSOPHY<br>
Visionating, Innovating and creating for the convergence of Art, Technology and Society. <br><br>प्रतिबोधविदितं मतं हि अमृतत्वं विन्दते। आत्मना वीर्यं विन्दते। विद्यया अमृतं विन्दते ॥ <a data-tooltip-position="top" aria-label="https://upanishads.org.in/upanishads/2/2/4" rel="noopener nofollow" class="external-link" href="https://upanishads.org.in/upanishads/2/2/4" target="_blank">from</a><br>
When It is known by perception that reflects It, then one has the thought of It, for one finds immortality; by the self one finds the force to attain and by the knowledge one finds immortality.<br><br>]]></description><link>index.html</link><guid isPermaLink="false">Index.md</guid><dc:creator><![CDATA[Surjit Das]]></dc:creator><pubDate>Sun, 20 Jul 2025 07:43:30 GMT</pubDate><enclosure url="attachments/neurongraph_logo_2.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="attachments/neurongraph_logo_2.png"&gt;&lt;/figure&gt;</content:encoded></item></channel></rss>